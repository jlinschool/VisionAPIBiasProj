{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSCJmqtEowGy"
      },
      "source": [
        "# **QTM 250 Homework 4**\n",
        "## Racial Variation Bias in API\n",
        "\n",
        "Sarah Bekele, Emma Seto, Jennifer Dam, Stellar Xu, Jonathan Lin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-v2fBR8icdB"
      },
      "source": [
        "## **Introduction**\n",
        "\n",
        "In recent years, people have increasingly used machine learning (ML) algorithms in various applications, such as facial and emotion recognition. These recognition tools have manifested themselves in various facets of general use such as Apple products as well as airport security checkpoints. ([Source](https://www.thalesgroup.com/en/markets/digital-identity-and-security/government/biometrics/facial-recognition)) With the seemingly already extensive use of this new technology, they have undoubtedly been trained on a huge amount of image data. \n",
        "\n",
        "However, there has been growing concern that these algorithms may be biased, particularly concerning skin color and gender identities. As a result of these biased samples, when the ML and AI models attempt to recognize those who do not fit that mold of being a white male, they give inaccurate results. Additionally, given that people of different skin tones may have different cultural and social experiences that influence their emotional expressions, it is important to investigate how ML algorithms, such as Google's ML vision API, gauge emotions (joy, sorrow, anger, surprise) across different skin tones. To give a little more background on the nature of this issue, a fundamental definition of model bias can be seen as the difference between model prediction and the ground truth ([Source](https://www.bmc.com/blogs/bias-variance-machine-learning/)). A model with higher bias would likely not match any facet of the dataset very closely and would therefore, predict features very poorly while a low bias model would not be generalizable to outside data. As mentioned before, the descrepancy between the training data and the general population is a large concern pertaining to bias. \n",
        "\n",
        "This project aims to explore the extent to which Google's ML vision API accurately recognizes emotions in people of varying skin tones and the potential implications of any biases in its performance. By doing so, we hope to contribute to the ongoing discussion on ensuring that these algorithms are developed and used fairly and equitably. The conversation of data collection and build of training data is an incredibly important. Though we can acknowledge that there are many ways for a certain model to be biased based on both its developer origins and training tendencies, this project specifies an investigation into the explicit accuracy of the model instead of the causes of the bias. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnaKFIxNcW_T"
      },
      "source": [
        "## **Methods**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEDPtswkcf68"
      },
      "source": [
        "This project utilized images selected off of the internet as well as storage solutions offered by Google Cloud by way of buckets ([Details](https://cloud.google.com/storage)). These images not only varied in racial identity but also in perceived facial expressions. Each combination of these variables were evaluated with an adequate amount of images. Further code was used to process the image data as well as evaluating it using Google's Cloud Vision API in the Google Cloud Console. The results were extracted from the returned JSON file in the console editor and placed into a spreadsheet. This machine learning model is already utilized by many high-profile firms as well as public higher-education institutions which suggests its expected relative high rate of accuracy. \n",
        "\n",
        "### Dataset\n",
        "\n",
        "The dataset is stored in a Cloud Bucket at this [link](https://console.cloud.google.com/storage/browser/images-hw4;tab=objects?prefix=&forceOnObjectsSortingFiltering=false). It consists of 40 depictions of three different races (asian, black, and white) and five different facial expressions (anger, control, joy, sorrow, and surprise). The asian race was further divided into east and south asian. Within these differing categories, there was also a distinction made between male and female subjects. Each race category had two individuals with the same facial expression (1 male and 1 female) and there were four identical groups in total. The datapoints were then pre-labeled by experimenters through human emotional perception with the intention of comparing these with the prediction of the algorithm. These images were also labeled accordingly while being stored in the bucket. \n",
        "\n",
        "### Coding Workflow\n",
        "\n",
        "The coding strategy that was deployed included the use of the Cloud Console as well as the Code Editor. An API key was first deployed in the Google Cloud ([Directions](https://developers.google.com/maps/documentation/maps-static/get-api-key#:~:text=Go%20to%20the%20Google%20Maps%20Platform%20%3E%20Credentials%20page.&text=On%20the%20Credentials%20page%2C%20click,Click%20Close.)). This was then used in conjunction with the following code in the Cloud Console Terminal Window:\n",
        "\n",
        "![ExportAPIKey](https://github.com/jlinschool/VisionAPIBiasProj/blob/main/images/CloudAPICall.png?raw=true)\n",
        "\n",
        "Then, the Open Editor feature was used to run the requests command. The \"source\" line is the only one that changes in between images and since the bucket will be the same, the only variation is the image name. This was changed for each image in the dataset.\n",
        "\n",
        "![CommandBody](https://github.com/jlinschool/VisionAPIBiasProj/blob/main/images/BodyCommand.png?raw=true)\n",
        "\n",
        "Finally, the following command was used in the Cloud Console to code an API call with the aforementioned request. This then gave the desired output. \n",
        "\n",
        "![Output](https://github.com/jlinschool/VisionAPIBiasProj/blob/main/images/Output.png?raw=true)\n",
        "\n",
        "### Architecture Diagram\n",
        "\n",
        "The following diagram was created using Google Cloud's Architecture Diagramming Tool and depicts the general flow of information in this project. \n",
        "\n",
        "![FlowMap](https://github.com/jlinschool/VisionAPIBiasProj/blob/main/BasicFlowMap.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Results**\n",
        "\n",
        "Disclaimer: Other facial features including the data for the nose, left eye, right eye, and many others were shown, but here we only show part of the results. After running and following the code in the methods section above, you will be able to see the full results. \n",
        "\n"
      ],
      "metadata": {
        "id": "GsN6Njw05UET"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![imagelabel](https://github.com/jlinschool/VisionAPIBiasProj/blob/main/images/All.png?raw=true)\n",
        "\n",
        "Figure 1. Column graph depicting the Cloud Vision API average detection confidence for emotions (Joy, Anger, Sorrow, and Surprise) across different races\n"
      ],
      "metadata": {
        "id": "6J1FjJcOLgJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![imagelabel](https://github.com/jlinschool/VisionAPIBiasProj/blob/main/images/RaceConfidenceScore.png?raw=true)\n",
        "\n",
        "Figure 2. Overall average detection confidence of the API for each race yielding very similar results"
      ],
      "metadata": {
        "id": "Eg0rhhVFLpy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![imagelabel](https://github.com/jlinschool/VisionAPIBiasProj/blob/main/images/GenderRaceConfidenceScore.png?raw=true)\n",
        "\n",
        "Figure 3. Column graph depicting the average detection confidence between genders within each race category"
      ],
      "metadata": {
        "id": "YUpSwn-ELSO2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Example Outputs\n",
        "\n",
        " This is the output in google console of white male of surprise emotion.\n",
        " ![Output](https://github.com/jlinschool/VisionAPIBiasProj/blob/main/images/Result1.png?raw=TRUE)   \n",
        "\n",
        "![Output](https://github.com/jlinschool/VisionAPIBiasProj/blob/main/images/Result2.png?raw=TRUE)  "
      ],
      "metadata": {
        "id": "v15xNlLC73i4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Analysis**\n",
        "\n",
        "The detection confidence score shows the probability of the image being detected correctly by the Cloud Vision API algorithm. Looking at all three figures it can be seen that the average detection confidence is incredibly similar across all comparisons. Based on Figure 1 the emotion \"surprise\" has the least amount of variation while joy and anger have the most variation, also small. Figure 2 shows that the API was able to obtain high detection confidence regardless of race, gender, and facial expression. The results are as followed:\n",
        "\n",
        "#### **Overall Detection Confidence**\n",
        "*   White:0.96953125\n",
        "*   Black:0.977734375\n",
        "*   East Asian:0.9875\n",
        "*   South Asian:0.98828125\n",
        "\n",
        "Figure 3 takes a closer look at each race category individually by analyzing the confidence for each gender (men & women.) Once again, the API yielded high confidence within each category. For men the highest confidence was    98.98% and this was found in the sample for South Asian individuals. The lowest confidence for men was 98.05% and was found in the sample for Black individuals. For women, the highest confidence was 99.12% and this was found in the sample for East Asian individuals. The lowest confidence for men was 95.08% and this was found in the sample for Black individuals. The picture of the output shows how the API analyzed each photo and labeled each emotion  with \"Very Likely\" and \"Very Unlikely.\" Four types of expressions were considered for each photo and the API left only 1 \"very likely\" signaling which expression was depicted in that photo. Overall, the Cloud Vision API in our experiment was highly accurate.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hJvIjaLc-5wl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Discussion**\n",
        "\n",
        "From the analysis, it appears that race and gender do not impact the effectiveness of Google Cloud's ML Vision API at detecting emotions (joy, anger, sorrow, and surprise). We conducted this study because of Google's ML APIs' past concerning biases toward people of color [(Source)](https://algorithmwatch.org/en/google-vision-racism/). However, since the conversation about training ML became sensationalized in 2019 and 2020, Google has had ample time to work on their ML APIs and teach them using more representative samples of the world's population. \n",
        "\n",
        "It is important to note that this study has limitations. Firstly, the pictures we selected for our were all images we found available to us on the internet, and because of this lack of control, they used exaggerated facial expressions. This may have influenced the API's ability to recognize emotions accurately. In future studies, it may be better to have the researchers take the photos themselves or to find pictures with more subtle facial expressions. \n",
        "\n",
        "Additionally, it is important to note that our study used a relatively small sample size, with only two images for each race in each emotion category (40 photos in the dataset). To have more accurate results and know if these results can be generalized, it would be wise to have a more robust sample size in future studies.\n",
        "\n",
        "While the results of this study did not uncover any significant scandals or egregious biases, and thankfully so, we believe it is crucial to continue to poke at  ML and AI algorithms. As AI continues to become more involved in our daily lives, it is important to ensure these systems are not accidentally perpetuating harmful stereotypes or discrimination. And as facial recognition becomes more widespread, there is a need to ensure that it is being used ethically and with proper oversight. By continuing to examine and challenge these systems, we can work towards building a more just and equitable society."
      ],
      "metadata": {
        "id": "D94Th_Rn_HyQ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}